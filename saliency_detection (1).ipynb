{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "ZwjTaqkQECDF",
        "outputId": "8f19eef4-c4f9-4580-9878-4d5828fb618b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                               img_url  \\\n",
              "1114679353714016256  http://pbs.twimg.com/tweet_video_thumb/D3gi9MH...   \n",
              "1063020048816660480  http://pbs.twimg.com/ext_tw_video_thumb/106301...   \n",
              "1108927368075374593     http://pbs.twimg.com/media/D2OzhzHUwAADQjd.jpg   \n",
              "1114558534635618305  http://pbs.twimg.com/ext_tw_video_thumb/111401...   \n",
              "1035252480215592966     http://pbs.twimg.com/media/Dl30pGIU8AAVGxO.jpg   \n",
              "\n",
              "                        labels  \\\n",
              "1114679353714016256  [4, 1, 3]   \n",
              "1063020048816660480  [5, 5, 5]   \n",
              "1108927368075374593  [0, 0, 0]   \n",
              "1114558534635618305  [1, 0, 0]   \n",
              "1035252480215592966  [1, 0, 1]   \n",
              "\n",
              "                                                             tweet_url  \\\n",
              "1114679353714016256  https://twitter.com/user/status/11146793537140...   \n",
              "1063020048816660480  https://twitter.com/user/status/10630200488166...   \n",
              "1108927368075374593  https://twitter.com/user/status/11089273680753...   \n",
              "1114558534635618305  https://twitter.com/user/status/11145585346356...   \n",
              "1035252480215592966  https://twitter.com/user/status/10352524802155...   \n",
              "\n",
              "                                                            tweet_text  \\\n",
              "1114679353714016256       @FriskDontMiss Nigga https://t.co/cAsaLWEpue   \n",
              "1063020048816660480     My horses are retarded https://t.co/HYhqc6d5WN   \n",
              "1108927368075374593  “NIGGA ON MA MOMMA YOUNGBOY BE SPITTING REAL S...   \n",
              "1114558534635618305  RT xxSuGVNGxx: I ran into this HOLY NIGGA TODA...   \n",
              "1035252480215592966  “EVERYbody calling you Nigger now!” https://t....   \n",
              "\n",
              "                                            labels_str  \n",
              "1114679353714016256      [Religion, Racist, Homophobe]  \n",
              "1063020048816660480  [OtherHate, OtherHate, OtherHate]  \n",
              "1108927368075374593        [NotHate, NotHate, NotHate]  \n",
              "1114558534635618305         [Racist, NotHate, NotHate]  \n",
              "1035252480215592966          [Racist, NotHate, Racist]  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-af32b515-4021-40c9-bf2f-5af00f1e5f3f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>img_url</th>\n",
              "      <th>labels</th>\n",
              "      <th>tweet_url</th>\n",
              "      <th>tweet_text</th>\n",
              "      <th>labels_str</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1114679353714016256</th>\n",
              "      <td>http://pbs.twimg.com/tweet_video_thumb/D3gi9MH...</td>\n",
              "      <td>[4, 1, 3]</td>\n",
              "      <td>https://twitter.com/user/status/11146793537140...</td>\n",
              "      <td>@FriskDontMiss Nigga https://t.co/cAsaLWEpue</td>\n",
              "      <td>[Religion, Racist, Homophobe]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1063020048816660480</th>\n",
              "      <td>http://pbs.twimg.com/ext_tw_video_thumb/106301...</td>\n",
              "      <td>[5, 5, 5]</td>\n",
              "      <td>https://twitter.com/user/status/10630200488166...</td>\n",
              "      <td>My horses are retarded https://t.co/HYhqc6d5WN</td>\n",
              "      <td>[OtherHate, OtherHate, OtherHate]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1108927368075374593</th>\n",
              "      <td>http://pbs.twimg.com/media/D2OzhzHUwAADQjd.jpg</td>\n",
              "      <td>[0, 0, 0]</td>\n",
              "      <td>https://twitter.com/user/status/11089273680753...</td>\n",
              "      <td>“NIGGA ON MA MOMMA YOUNGBOY BE SPITTING REAL S...</td>\n",
              "      <td>[NotHate, NotHate, NotHate]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1114558534635618305</th>\n",
              "      <td>http://pbs.twimg.com/ext_tw_video_thumb/111401...</td>\n",
              "      <td>[1, 0, 0]</td>\n",
              "      <td>https://twitter.com/user/status/11145585346356...</td>\n",
              "      <td>RT xxSuGVNGxx: I ran into this HOLY NIGGA TODA...</td>\n",
              "      <td>[Racist, NotHate, NotHate]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1035252480215592966</th>\n",
              "      <td>http://pbs.twimg.com/media/Dl30pGIU8AAVGxO.jpg</td>\n",
              "      <td>[1, 0, 1]</td>\n",
              "      <td>https://twitter.com/user/status/10352524802155...</td>\n",
              "      <td>“EVERYbody calling you Nigger now!” https://t....</td>\n",
              "      <td>[Racist, NotHate, Racist]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-af32b515-4021-40c9-bf2f-5af00f1e5f3f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-af32b515-4021-40c9-bf2f-5af00f1e5f3f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-af32b515-4021-40c9-bf2f-5af00f1e5f3f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6e39d4a0-b8e4-42db-ad27-b72fc74ab311\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6e39d4a0-b8e4-42db-ad27-b72fc74ab311')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6e39d4a0-b8e4-42db-ad27-b72fc74ab311 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Load the JSON file\n",
        "with open('MMHS150K_GT.json') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Convert the data to a pandas DataFrame\n",
        "df = pd.DataFrame(data).T  # Transpose the data to get proper structure\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sexist_tweets = df[df['labels_str'].apply(lambda x: 'Sexist' in x)]\n"
      ],
      "metadata": {
        "id": "8WLAJlUUEs_H"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "racist_tweets = df[df['labels_str'].apply(lambda x: 'Racist' in x)]"
      ],
      "metadata": {
        "id": "kYcIKx-nE9Qk"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "homophobic_tweets = df[df['labels_str'].apply(lambda x: 'Homophobe' in x)]"
      ],
      "metadata": {
        "id": "aAQaNqEUE_90"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "disability_keywords = ['retard', 'cripple', 'spaz', 'invalid', 'gimp', 'crazy', 'freak']\n",
        "disability_tweets = df[df['tweet_text'].str.contains('|'.join(disability_keywords), case=False, na=False)]\n"
      ],
      "metadata": {
        "id": "q3EexYC2FCsF"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subset_df = df.sample(frac=0.01)"
      ],
      "metadata": {
        "id": "Ssjf5Cy1WWlV"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize tweet text\n",
        "tokens = tokenizer(subset_df['tweet_text'].tolist(), padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "# Extract text embeddings\n",
        "outputs = model(**tokens)\n",
        "text_embeddings = outputs.last_hidden_state\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddfkjZm7FHtz",
        "outputId": "516ee063-54b1-4e7c-a270-ca2f996bb1a2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5-nwgQiX8jn",
        "outputId": "830c348c-7cab-4c38-8481-896ed9fdcab9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-ae5efkbp\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-ae5efkbp\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy (from clip==1.0)\n",
            "  Downloading ftfy-6.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (24.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.4.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.19.1+cu121)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2024.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
            "Downloading ftfy-6.3.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369489 sha256=adf8b54166d695e7954977fe8cfd0295bf3b85320bb2bb4f2c00311e7d0d950f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-5fqfsdyw/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n",
            "Successfully built clip\n",
            "Installing collected packages: ftfy, clip\n",
            "Successfully installed clip-1.0 ftfy-6.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import clip\n",
        "# from PIL import Image\n",
        "\n",
        "# # Load CLIP model\n",
        "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "# # Load and preprocess image\n",
        "# image = Image.open('/content/1023940590382268417.jpg')\n",
        "# image_input = preprocess(image).unsqueeze(0).to(device)\n",
        "\n",
        "# # Extract visual features\n",
        "# with torch.no_grad():\n",
        "#     image_features = model.encode_image(image_input)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqEQoULGFRsc",
        "outputId": "d09da360-f699-4f1d-ad69-a4c79bfacf1d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 338M/338M [00:04<00:00, 72.2MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import clip\n",
        "\n",
        "# Assuming 'text_embeddings' and 'image_features' are the embeddings for a tweet\n",
        "\n",
        "# Step 1: Define saliency points\n",
        "gender_saliency_points = [\n",
        "    \"bitch\", \"slut\", \"feminazi\", \"stay in the kitchen\", \"man up\", \"gold digger\",\n",
        "    \"women are property\", \"catfight\", \"misandrist\", \"alpha male\"\n",
        "]\n",
        "disability_saliency_points = [\n",
        "    \"retard\", \"retarded\", \"cripple\", \"crippled\", \"burden on society\",\n",
        "    \"worthless\", \"crazy\", \"spaz\", \"spastic\", \"freak\", \"vegetable\",\n",
        "    \"invalid\", \"better off dead\", \"gimp\"\n",
        "]\n",
        "race_saliency_points = [\n",
        "    \"nigger\", \"monkey\", \"go back to your country\", \"illegal\", \"terrorist\",\n",
        "    \"chink\", \"spic\", \"thug\", \"white supremacy\", \"genetic inferior\"\n",
        "]\n",
        "lgbtqia_saliency_points = [\n",
        "    \"faggot\", \"dyke\", \"tranny\", \"that's so gay\", \"it’s just a phase\",\n",
        "    \"sodomite\", \"no homo\", \"groomer\", \"burn in hell\", \"agenda\"\n",
        "]\n",
        "\n",
        "# Step 2: Encode saliency points using CLIP's text encoder\n",
        "def encode_saliency_points(points, model, device):\n",
        "    text_tokens = clip.tokenize(points).to(device)\n",
        "    with torch.no_grad():\n",
        "        text_features = model.encode_text(text_tokens)\n",
        "    return text_features\n",
        "\n",
        "# Encode saliency points for each dimension\n",
        "gender_features = encode_saliency_points(gender_saliency_points, clip_model, device)\n",
        "disability_features = encode_saliency_points(disability_saliency_points, clip_model, device)\n",
        "race_features = encode_saliency_points(race_saliency_points, clip_model, device)\n",
        "lgbtqia_features = encode_saliency_points(lgbtqia_saliency_points, clip_model, device)"
      ],
      "metadata": {
        "id": "thCfwYjkqWhO"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import clip\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize tweet text (replace with your actual text data)\n",
        "tweet_texts = subset_df['tweet_text'].tolist()  # Assuming subset_df['tweet_text'] contains your text data\n",
        "tokens = bert_tokenizer(tweet_texts, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "# Extract text embeddings (CLS token representation for the whole sentence)\n",
        "with torch.no_grad():\n",
        "    outputs = bert_model(**tokens)\n",
        "    text_embeddings = outputs.last_hidden_state[:, 0, :]  # Shape: (batch_size, 768)\n",
        "\n",
        "# Create a linear layer to project BERT embeddings to 512 dimensions\n",
        "projection_layer = nn.Linear(768, 512).to(device)\n",
        "\n",
        "# Project the text embeddings to 512 dimensions\n",
        "text_embeddings_projected = projection_layer(text_embeddings.to(device))  # Shape: (batch_size, 512)\n",
        "\n",
        "# Load CLIP model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "# Load and preprocess image (replace with your actual image path)\n",
        "image_path = '/content/1023942343202881536.jpg'\n",
        "image = Image.open(image_path)\n",
        "image_input = preprocess(image).unsqueeze(0).to(device)\n",
        "\n",
        "# Extract visual features\n",
        "with torch.no_grad():\n",
        "    image_features = clip_model.encode_image(image_input)  # Shape: (1, 512)\n",
        "\n",
        "# Compute cosine similarity between projected text embeddings and image features\n",
        "similarity = torch.nn.functional.cosine_similarity(text_embeddings_projected, image_features, dim=-1)\n",
        "\n",
        "# Print similarity\n",
        "print(similarity)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6oVOXbZsZIkR",
        "outputId": "39186427-db8b-47dc-8a70-b12bba6f38c0"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0.0021, -0.0132, -0.0248,  ..., -0.0115,  0.0064,  0.0010],\n",
            "       device='cuda:0', grad_fn=<SumBackward1>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lZ7jkByXqVhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# Define a projection layer to map 768-dimensional BERT embeddings to 512 dimensions\n",
        "projection_layer = nn.Linear(768, 512).to(device)\n",
        "\n",
        "# Project the text embeddings to match the dimension of CLIP embeddings (512)\n",
        "# Move text_embeddings to the same device as projection_layer before applying it\n",
        "projected_text_embeddings = projection_layer(text_embeddings.to(device))\n",
        "\n",
        "# Step 3: Compute cosine similarity for each dimension (with projected embeddings)\n",
        "def compute_similarity(text_embeddings, image_features, saliency_features):\n",
        "    # Initialize empty lists to store similarity scores\n",
        "    text_similarities = []\n",
        "    image_similarities = []\n",
        "\n",
        "    # Iterate over saliency points and compute similarity for each\n",
        "    for i in range(saliency_features.shape[0]):\n",
        "        # Select the current saliency point features\n",
        "        current_saliency_features = saliency_features[i].unsqueeze(0)  # Reshape to (1, 512)\n",
        "\n",
        "        # Calculate cosine similarity for text and image with the current saliency point\n",
        "        text_similarity = torch.nn.functional.cosine_similarity(text_embeddings, current_saliency_features, dim=-1)\n",
        "        image_similarity = torch.nn.functional.cosine_similarity(image_features, current_saliency_features, dim=-1)\n",
        "\n",
        "        # Append the similarity scores to the lists\n",
        "        text_similarities.append(text_similarity)\n",
        "        image_similarities.append(image_similarity)\n",
        "\n",
        "    # Stack the similarity scores into tensors\n",
        "    text_similarities = torch.stack(text_similarities)  # Shape: (num_saliency_points, num_text_embeddings)\n",
        "    image_similarities = torch.stack(image_similarities) # Shape: (num_saliency_points, num_image_features which is 1)\n",
        "\n",
        "    return text_similarities, image_similarities\n",
        "\n",
        "# Now you can compute similarity as before, but with projected text embeddings\n",
        "gender_text_sim, gender_image_sim = compute_similarity(projected_text_embeddings, image_features, gender_features)\n",
        "disability_text_sim, disability_image_sim = compute_similarity(projected_text_embeddings, image_features, disability_features)\n",
        "race_text_sim, race_image_sim = compute_similarity(projected_text_embeddings, image_features, race_features)\n",
        "lgbtqia_text_sim, lgbtqia_image_sim = compute_similarity(projected_text_embeddings, image_features, lgbtqia_features)"
      ],
      "metadata": {
        "id": "C6gMGOrpbsKy"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Summarize detected saliency points for each dimension\n",
        "def detect_saliency(similarity_scores, threshold=0.1):\n",
        "    return (similarity_scores > threshold).nonzero(as_tuple=True)[0].tolist()  # Indices of high similarity\n",
        "\n",
        "# Detect high-similarity points\n",
        "gender_saliency_detected = detect_saliency(gender_text_sim)\n",
        "disability_saliency_detected = detect_saliency(disability_text_sim)\n",
        "race_saliency_detected = detect_saliency(race_text_sim)\n",
        "lgbtqia_saliency_detected = detect_saliency(lgbtqia_text_sim)\n",
        "\n",
        "# Output saliency points detected in each dimension\n",
        "print(\"Gender Dimension - Saliency Points Detected:\", gender_saliency_detected)\n",
        "print(\"Disability Dimension - Saliency Points Detected:\", disability_saliency_detected)\n",
        "print(\"Race Dimension - Saliency Points Detected:\", race_saliency_detected)\n",
        "print(\"LGBTQIA+ Dimension - Saliency Points Detected:\", lgbtqia_saliency_detected)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RitvktHqcXj9",
        "outputId": "8e8d28fe-8f48-4ab3-ec6a-e016767a5a1c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gender Dimension - Saliency Points Detected: [7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n",
            "Disability Dimension - Saliency Points Detected: []\n",
            "Race Dimension - Saliency Points Detected: [4, 4, 4, 5, 9]\n",
            "LGBTQIA+ Dimension - Saliency Points Detected: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "06_HonyOdAcc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}